{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72b6fa66-beeb-4fdf-8de1-0bf6c1f17339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import MinBPE.TikToken\n",
    "import MinBPE.Codec\n",
    "import MinBPE.Types\n",
    "\n",
    "import qualified Data.Text.IO as TIO\n",
    "import qualified Data.Map as Map\n",
    "import qualified Data.ByteString as B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e1b8e5-574e-4ec8-88c6-a42c851c68b3",
   "metadata": {},
   "source": [
    "Loading TikToken files\n",
    "======================\n",
    "\n",
    "Load the cl100k_base encoding into a `Vocab`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19c35ca1-f3c3-4e0c-a29a-1000bd44cad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "v <- loadTikToken \"../example/cl100k_base.tiktoken\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d344c86-b2a0-407e-9ccd-21c225b8b69e",
   "metadata": {},
   "source": [
    "Now we can use the `Vocab`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec1cb0ee-0b79-47d3-bd89-67f90f812acd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100256"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Map.size v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2ede61f-71ac-408a-b498-776a03ad909a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"OpenAI's large language models (sometimes referred to as GPT's) process text using tokens, which are common sequences of characters found in a set of text. The models learn to understand the statistical relationships between these tokens, and excel at producing the next token in a sequence of tokens.\\n\\nYou can use the tool below to understand how a piece of text might be tokenized by a language model, and the total count of tokens in that piece of text.\\n\\nIt's important to note that the exact tokenization process varies between models. Newer models like GPT-3.5 and GPT-4 use a different tokenizer than previous models, and will produce different tokens for the same input text.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "decode v [\n",
    "    5109, 15836, 596, 3544, 4221, 4211, 320, 57753, 14183, 311, 439, 480, \n",
    "    2898, 596, 8, 1920, 1495, 1701, 11460, 11, 902, 527, 4279, 24630, 315,\n",
    "    5885, 1766, 304, 264, 743, 315, 1495, 13, 578, 4211, 4048, 311, 3619,\n",
    "    279, 29564, 12135, 1990, 1521, 11460, 11, 323, 25555, 520, 17843, 279,\n",
    "    1828, 4037, 304, 264, 8668, 315, 11460, 382, 2675, 649, 1005, 279, 5507,\n",
    "    3770, 311, 3619, 1268, 264, 6710, 315, 1495, 2643, 387, 4037, 1534, 555,\n",
    "    264, 4221, 1646, 11, 323, 279, 2860, 1797, 315, 11460, 304, 430, 6710, 315,\n",
    "    1495, 382, 2181, 596, 3062, 311, 5296, 430, 279, 4839, 4037, 2065, 1920,\n",
    "    35327, 1990, 4211, 13, 1561, 261, 4211, 1093, 480, 2898, 12, 18, 13, 20,\n",
    "    323, 480, 2898, 12, 19, 1005, 264, 2204, 47058, 1109, 3766, 4211, 11, 323,\n",
    "    690, 8356, 2204, 11460, 369, 279, 1890, 1988, 1495, 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "deda012c-b3b3-422f-babd-24ae8775bc74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "お誕生日おめでとう"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TIO.putStrLn $ decode v [33334, 45918, 243, 21990, 9080, 33334, 62004, 16556, 78699]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88578e7-a945-4bca-ba8d-68249c175c13",
   "metadata": {},
   "source": [
    "We can convert the `Vocab` to `Vector` format to use the optimized decoders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbe5abd7-e803-445d-ac1c-98d17bb447db",
   "metadata": {},
   "outputs": [],
   "source": [
    "vv = vocabToVector v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ce85754-c4f5-4cf0-82b1-d5bb18a3fe49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"tiktoken is great!\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "decodeVec vv [83, 1609, 5963, 374, 2294, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b1454e-c0c8-44f6-a7ce-762906c2a2f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"antidisestablishmentarianism\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "decodeVec vv [519, 85342, 34500, 479, 8997, 2191]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f444cd6-6f80-4dd7-a8a2-652d199b1970",
   "metadata": {},
   "source": [
    "Writing TikToken files\n",
    "======================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dde5527-9bb8-4e26-86ea-c7f76c77095f",
   "metadata": {},
   "outputs": [],
   "source": [
    "writeTikToken \"../example/cl100k_base_copy.tiktoken\" v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31f3473-7879-4a3a-b7d2-0001ebf94268",
   "metadata": {},
   "source": [
    "Compare the written file to the original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b383ed-7ee4-4006-bb8d-2061a5d4f390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "original <- B.readFile \"../example/cl100k_base.tiktoken\"\n",
    "copy <- B.readFile \"../example/cl100k_base_copy.tiktoken\"\n",
    "original == copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9154275b-9e35-4d5e-a217-ff1cff306115",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Haskell",
   "language": "haskell",
   "name": "haskell"
  },
  "language_info": {
   "codemirror_mode": "ihaskell",
   "file_extension": ".hs",
   "mimetype": "text/x-haskell",
   "name": "haskell",
   "pygments_lexer": "Haskell",
   "version": "9.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
